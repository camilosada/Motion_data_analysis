{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondadbd63e3006f6496f9e4024d92b65070b",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from scipy import stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "104692\n104692\n"
     ]
    }
   ],
   "source": [
    "# loading metadata file\n",
    "filePath = 'filePath'\n",
    "df = pd.read_csv(filePath + '.csv').drop_duplicates()\n",
    "# Pre-processing to filter out inconsistent data\n",
    "df = df[df['Number of Occupants'] < df['Number of Occupants'].quantile(.999)]\n",
    "df = df[df['Number of Floors'] < df['Number of Floors'].quantile(.999)]\n",
    "df = df[df['Floor Area [ft2]'] < df['Floor Area [ft2]'].quantile(.999)]\n",
    "df = df[df['Number of Remote Sensors'] < df['Number of Remote Sensors'].quantile(.999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "# List all files in directory using pathlib\n",
    "basepath = Path(\"path\")\n",
    "files_in_basepath = (entry for entry in basepath.iterdir() if entry.is_file())\n",
    "for item in files_in_basepath:\n",
    "    lista.append(item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_FileName = [s.replace('.csv', '') for s in lista]  # to have only the name of the file\n",
    "df_in_2019 = df[df.Identifier.isin(lista_FileName)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_in_2019[(df_in_2019['Number of Occupants'] == 1 ) & (df_in_2019['Number of Remote Sensors'] >= 3)]"
   ]
  },
  {
   "source": [
    "## Selection of Identifier with certains characteristics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(916, 22)\n(3802, 22)\n(2418, 22)\n(2813, 22)\n"
     ]
    }
   ],
   "source": [
    "df_in_2019= df_in_2019[(df_in_2019['Country']=='US') | (df_in_2019['Country']=='CA')]\n",
    "df1 = df_in_2019[(df_in_2019['Number of Occupants'] == 1 ) & (df_in_2019['Number of Remote Sensors'] >= 3)]\n",
    "df2 = df_in_2019[(df_in_2019['Number of Occupants'] == 2 ) & (df_in_2019['Number of Remote Sensors'] >= 3)]\n",
    "df3 = df_in_2019[(df_in_2019['Number of Occupants'] == 3 ) & (df_in_2019['Number of Remote Sensors'] >= 3)]\n",
    "df4 = df_in_2019[(df_in_2019['Number of Occupants'] == 4 ) & (df_in_2019['Number of Remote Sensors'] >= 3)]\n",
    "No_RS=3\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [cols]\n",
    "mainPath ='mainPath'"
   ]
  },
  {
   "source": [
    "## Drop repeated DateTime of each Identifier, mean activation = activation/ number of sensors with value at that time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF1\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a1=df1.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a1:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "    df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "    df_motion['DateTime'] = df['DateTime']\n",
    "    df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "    df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "    df_motion[['DateTime','mean_activation']].to_csv('path'+filename+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF2\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a2=df2.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a2:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "    df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "    df_motion['DateTime'] = df['DateTime']\n",
    "    df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "    df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "    df_motion[['DateTime','mean_activation']].to_csv('path/'+filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF3\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a3=df3.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a3:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "    df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "    df_motion['DateTime'] = df['DateTime']\n",
    "    df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "    df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "    df_motion[['DateTime','mean_activation']].to_csv('path'+filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF4\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a4=df4.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a4:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "    df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "    df_motion['DateTime'] = df['DateTime']\n",
    "    df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "    df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "    df_motion[['DateTime','mean_activation']].to_csv('path'+filename+'.csv')"
   ]
  },
  {
   "source": [
    "## Mean value for Identifiers with 1,2,3,4 occup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('path/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('path/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df1[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('path/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('path/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df2[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll2=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll2.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df3[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll3=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll3.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df4[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll4=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll4.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "source": [
    "## filter ID with less than 245 days with information (245*24*12 cant samples each 5 min in 245 days) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF1\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a1=df1.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a1:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    if  len(df_motion[df_motion['working_sensors'] > 3]) >= 245*24*12:\n",
    "\n",
    "        df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "        df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "        df_motion['DateTime'] = df['DateTime']\n",
    "        df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "        df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "        df_motion[['DateTime','mean_activation']].to_csv('/'+filename+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv(/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df1[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll_1=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll_1.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF2\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a2=df2.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a2:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    if  len(df_motion[df_motion['working_sensors'] > 3]) >= 245*24*12:\n",
    "\n",
    "        df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "        df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "        df_motion['DateTime'] = df['DateTime']\n",
    "        df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "        df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "        df_motion[['DateTime','mean_activation']].to_csv('/'+filename+'.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df2[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll_2=result_g.compute()\n",
    "ll_compu=pd.DataFrame(ll_2.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/1000 \n",
    "ll_compu[[\"('mean_activation', 'mean')\",\"('mean_activation', 'count')\"]].plot(figsize=(25,8))"
   ]
  },
  {
   "source": [
    "## Take only dateTimes with information from at least 4 sensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF1\n",
    "\n",
    "# Take all Identifiers of df, to after open them\n",
    "a1=df1.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a1:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    if  len(df_motion[df_motion['working_sensors'] > 3]) >= 245*24*12:\n",
    "\n",
    "        df_motion = df_motion[df_motion['working_sensors'] > 3] # Take only dateTimes with information from at least 4 sensors\n",
    "        df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "        df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "\n",
    "        df_motion['DateTime'] = df['DateTime']\n",
    "        df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "\n",
    "        df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "        df_motion[['DateTime','mean_activation']].to_csv('/'+filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('/*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df1[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll_1_1=result_g.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ll_1_1.to_records()).set_index('DateTime').to_csv('')\n",
    "ll_compu=pd.read_csv('').set_index('DateTime') \n",
    "ax=ll_compu[[\"('mean_activation', 'count')\"]].plot(figsize=(20,6))\n",
    "ax.legend(['Number of Identifiers'], fontsize=16)\n",
    "ax.set_title('1 occupant at least 245 days, DateTime at least from 4 sensors',fontsize=16)\n"
   ]
  },
  {
   "source": [
    "# Filter days that more than 70% identifiers have no data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ll_1_1=pd.read_csv('')\n",
    "ll_compu_1 = pd.DataFrame(ll_1_1.to_records()).set_index('DateTime').drop('index',axis=1)\n",
    "days_nan1 = pd.DataFrame(pd.to_datetime(ll_compu_1[ll_compu_1[\"('mean_activation', 'count')\"] > ((ll_compu_1[[\"('mean_activation', 'count')\"]])*0.7).max().iloc[0]].reset_index()['DateTime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF1\n",
    "# Take all Identifiers of df, to after open them\n",
    "a1=df1.loc[:,'Identifier'].tolist()\n",
    "path=mainPath\n",
    "\n",
    "for filename in a1:\n",
    "    df = pd.read_csv(mainPath+filename+'.csv')\n",
    "\n",
    "    df_motion = df.loc[:, df.columns.str.contains('_Motion')]\n",
    "    df_motion['working_sensors'] = 11-df_motion.isna().sum(axis=1)\n",
    "\n",
    "    if  len(df_motion[df_motion['working_sensors'] > 3]) >= 245*24*12:\n",
    "        pp1=df_motion\n",
    "        df_motion = df_motion[df_motion['working_sensors'] > 3] # Take only dateTimes with information from at least 4 sensors\n",
    "        df_motion['activation'] = df_motion.loc[:, df_motion.columns.str.contains('_Motion')].sum(axis=1)\n",
    "        df_motion['mean_activation'] = df_motion['activation']/df_motion['working_sensors'] \n",
    "        \n",
    "        df_motion['DateTime'] = df['DateTime']\n",
    "        df_motion['DateTime'] = pd.to_datetime(df_motion['DateTime'])\n",
    "        df_motion.drop_duplicates('DateTime', inplace=True)\n",
    "        \n",
    "        df_motion = pd.merge(df_motion, days_nan1['DateTime'], on='DateTime')\n",
    "        \n",
    "        df_motion[['DateTime','mean_activation']].to_csv('/'+filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=dd.read_csv('*', parse_dates=['DateTime'], include_path_column=True).drop('Unnamed: 0',axis=1)\n",
    "ff['path']=ff['path'].str.replace('/', '')\n",
    "ff['path']=ff['path'].str.replace('.csv', '')\n",
    "\n",
    "ff_columns = ['path', 'mean_activation', 'DateTime']\n",
    "result = dd.merge(df1[['Identifier','Number of Remote Sensors']], ff[ff_columns], left_on='Identifier', right_on='path').drop(['path'],axis=1)\n",
    "result_g = result[['mean_activation','DateTime']].groupby(['DateTime']).agg(['mean', 'std', 'count'])\n",
    "ll_1_1_filtro=result_g.compute()\n",
    "\n",
    "ll_compu=pd.DataFrame(ll_1_1_filtro.to_records()).set_index('DateTime')\n",
    "ll_compu[[\"('mean_activation', 'count')\"]]=ll_compu[[\"('mean_activation', 'count')\"]]/np.max(ll_compu[[\"('mean_activation', 'count')\"]]) \n",
    "ll_compu[[\"('mean_activation', 'mean')\",\"('mean_activation', 'count')\"]].plot(figsize=(25,8), title='1 occupant at least 245 days, DateTime at least from 4 sensors, 70% of identifiers', color=['#1f77b4','g'])\n"
   ]
  },
  {
   "source": [
    "#  filtering by percentiles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "lista1 = []\n",
    "# List all files in directory using pathlib\n",
    "basepath = Path(\"_1/\")\n",
    "files_in_basepath = (entry for entry in basepath.iterdir() if entry.is_file())\n",
    "for item in files_in_basepath:\n",
    "    lista1.append(item.name)\n",
    "lista_FileName1 = [s.replace('.csv', '') for s in lista1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_one = '1/' # use your path\n",
    "all_files_read1 = []\n",
    "\n",
    "for filename in lista_FileName1:\n",
    "    df_p = pd.read_csv(path_one+filename+'.csv', index_col=None, header=0)\n",
    "    \n",
    "    df_p['date'] = pd.to_datetime(df_p['DateTime']).dt.date\n",
    "    df_p = df_p[['date','mean_activation']].groupby('date').mean()\n",
    "    df_p['path'] = filename\n",
    "    all_files_read1.append(df_p)\n",
    "    \n",
    "frame_all_files1 = pd.concat(all_files_read1, axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_all_files1.to_csv('es.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_all_files1=pd.read_csv('es.csv')\n",
    "percentile_min = frame_all_files1[['date','mean_activation']].groupby('date').agg(lambda x: np.percentile(x, 20))\n",
    "percentile_min = pd.DataFrame(percentile_min)\n",
    "percentile_max =frame_all_files1[['date','mean_activation']].groupby('date').agg(lambda x: np.percentile(x, 80))\n",
    "percentile_max=pd.DataFrame(percentile_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group11 = frame_all_files1[['date','mean_activation','path']].groupby('path')\n",
    "li11_percentile = []\n",
    "complete=0\n",
    "incomplete=0\n",
    "for key, g_dataframe in group11:\n",
    "    if len(g_dataframe)>1:# take off this condition\n",
    "        fds_max=(percentile_max.reset_index()[['mean_activation','date']].merge(g_dataframe[['mean_activation','date']], how='outer', on='date',suffixes=('_percentilemax', '_df')))\n",
    "        fds_max_min=(percentile_min.reset_index()[['mean_activation','date']].merge(fds_max[['mean_activation_percentilemax','date','mean_activation_df']], how='outer', on='date'))\n",
    "        df_per_minmmax = fds_max_min[(fds_max_min['mean_activation_percentilemax']>fds_max_min['mean_activation_df']) & (fds_max_min['mean_activation']<fds_max_min['mean_activation_df'])]# \n",
    "       \n",
    "        if len(df_per_minmmax)>200:\n",
    "            complete=complete+1\n",
    "            li11_percentile.append(g_dataframe['path'].iloc[0])\n",
    "            \n",
    "        else:\n",
    "            incomplete=incomplete+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(li11_percentile).to_csv('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "466\n56\n"
     ]
    }
   ],
   "source": [
    "group11 = frame11[['DateTime','activation','path']].groupby('path')\n",
    "li11_percentile = []\n",
    "complete=0\n",
    "incomplete=0\n",
    "for key, g_dataframe in group11:\n",
    "    if len(g_dataframe)>200:\n",
    "\n",
    "        fds_max=(percentile_max.reset_index()[['activation','DateTime']].merge(g_dataframe[['activation','DateTime']], how='outer', on='DateTime',suffixes=('_percentilemax', '_df')))\n",
    "        fds_max_min=(percentile_min.reset_index()[['activation','DateTime']].merge(fds_max[['activation_percentilemax','DateTime','activation_df']], how='outer', on='DateTime'))\n",
    "        df_per_minmmax = fds_max_min[(fds_max_min['activation_percentilemax']>fds_max_min['activation_df']) & (fds_max_min['activation']<fds_max_min['activation_df'])]# \n",
    "       \n",
    "        if len(df_per_minmmax)>150:\n",
    "            complete=complete+1\n",
    "            li11_percentile.append(g_dataframe)\n",
    "        else:\n",
    "            incomplete=incomplete+1\n",
    "       \n",
    "print(complete)\n",
    "print(incomplete)\n",
    "frame111 = pd.concat(li11_percentile,  ignore_index=False)\n",
    "frame111=frame111.reset_index()\n",
    "ax1 = frame111[['DateTime','activation']].groupby('DateTime').mean().plot(figsize=(25,8),title='Identifiers with 1 occupant,4 sensors, from US')\n",
    "frame111[['DateTime','activation']].groupby('DateTime').std().plot(ax=ax1)\n",
    "ax1.set_ylabel('10% trimmed mean of sum sensors values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}